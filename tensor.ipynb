{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlN1UALRhkTjQlVdkb4Klp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nada-Naveesh/Tensorflow/blob/main/tensor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1CgdA8vZ--a",
        "outputId": "b99c1d4c-1b52-4aca-9ab4-380a5aa2f462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor-A = tf.Tensor(2, shape=(), dtype=int32)\n",
            "Tensor-B = tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
            "Tensor-C = tf.Tensor(\n",
            "[[1 2 3]\n",
            " [4 5 6]], shape=(2, 3), dtype=int32)\n",
            "Tensor-D = tf.Tensor(\n",
            "[[[ 1  2  3]\n",
            "  [ 4  5  6]]\n",
            "\n",
            " [[ 7  8  9]\n",
            "  [10 11 12]]\n",
            "\n",
            " [[13 14 15]\n",
            "  [16 17 18]]], shape=(3, 2, 3), dtype=int32)\n",
            "Rank of tensor-a is  tf.Tensor(0, shape=(), dtype=int32) and it is a scalar.\n",
            "Rank of tensor-b is  tf.Tensor(1, shape=(), dtype=int32) and it is a vector.\n",
            "Rank of tensor-c is  tf.Tensor(2, shape=(), dtype=int32) and it is a matrix.\n",
            "Rank of tensor-d is  tf.Tensor(3, shape=(), dtype=int32) and it is a 3-D tensor.\n"
          ]
        }
      ],
      "source": [
        "#write a program to find the order and type of a tensor.\n",
        "import tensorflow as tf\n",
        "a=tf.constant(2)\n",
        "b=tf.constant([1,2,3])\n",
        "c=tf.constant([[1,2,3],[4,5,6]])\n",
        "d=tf.constant([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]],[[13,14,15],[16,17,18]]])\n",
        "print('Tensor-A =',a)\n",
        "print('Tensor-B =',b)\n",
        "print('Tensor-C =',c)\n",
        "print('Tensor-D =',d)\n",
        "print('Rank of tensor-a is ',tf.rank(a),'and it is a scalar.')\n",
        "print('Rank of tensor-b is ',tf.rank(b),'and it is a vector.')\n",
        "print('Rank of tensor-c is ',tf.rank(c),'and it is a matrix.')\n",
        "print('Rank of tensor-d is ',tf.rank(d),'and it is a 3-D tensor.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Arithmetic, Comparative - element wise operations\n",
        "\n",
        "import tensorflow as tf\n",
        "a=tf.constant([[1,2,3],[4,5,6]])\n",
        "b=tf.constant([[7,8,9],[10,11,12]])\n",
        "print('Element in Tensor-A =', a)\n",
        "print('Element in Tensor-B =', b)\n",
        "print('Element wise sum of tensors=',tf.add(a,b))\n",
        "print('Element wise difference betweeen two tensors =',tf.subtract(a,b))\n",
        "print('Element wise product of two Tensors =',tf.multiply(a,b))\n",
        "print('Element wise divisions of two tensors =',tf.divide(a,b))\n",
        "print('Element wise power of  two tensors =', tf.pow(a,b))\n",
        "\n",
        "#Comparative Operators\n",
        "\n",
        "print('Check for the element wise equality of tensors :',tf.equal(a,b))\n",
        "print('Check for the element wise not_equality of tensors :',tf.not_equal(a,b))\n",
        "print('Check for the element wise greater value of tensors :',tf.greater(a,b))\n",
        "print('Check for the element wise less value of tensors :',tf.less(a,b))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stvr91OPc0e_",
        "outputId": "bc03b456-6b16-49d8-8ea8-1a776cea7718"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Element in Tensor-A = tf.Tensor(\n",
            "[[1 2 3]\n",
            " [4 5 6]], shape=(2, 3), dtype=int32)\n",
            "Element in Tensor-B = tf.Tensor(\n",
            "[[ 7  8  9]\n",
            " [10 11 12]], shape=(2, 3), dtype=int32)\n",
            "Element wise sum of tensors= tf.Tensor(\n",
            "[[ 8 10 12]\n",
            " [14 16 18]], shape=(2, 3), dtype=int32)\n",
            "Element wise difference betweeen two tensors = tf.Tensor(\n",
            "[[-6 -6 -6]\n",
            " [-6 -6 -6]], shape=(2, 3), dtype=int32)\n",
            "Element wise product of two Tensors = tf.Tensor(\n",
            "[[ 7 16 27]\n",
            " [40 55 72]], shape=(2, 3), dtype=int32)\n",
            "Element wise divisions of two tensors = tf.Tensor(\n",
            "[[0.14285714 0.25       0.33333333]\n",
            " [0.4        0.45454545 0.5       ]], shape=(2, 3), dtype=float64)\n",
            "Element wise power of  two tensors = tf.Tensor(\n",
            "[[          1         256       19683]\n",
            " [    1048576    48828125 -2118184960]], shape=(2, 3), dtype=int32)\n",
            "Check for the element wise equality of tensors : tf.Tensor(\n",
            "[[False False False]\n",
            " [False False False]], shape=(2, 3), dtype=bool)\n",
            "Check for the element wise inequality of tensors : tf.Tensor(\n",
            "[[ True  True  True]\n",
            " [ True  True  True]], shape=(2, 3), dtype=bool)\n",
            "Check for the element wise greater value of tensors : tf.Tensor(\n",
            "[[False False False]\n",
            " [False False False]], shape=(2, 3), dtype=bool)\n",
            "Check for the element wise less value of tensors : tf.Tensor(\n",
            "[[ True  True  True]\n",
            " [ True  True  True]], shape=(2, 3), dtype=bool)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#write a program to find the value of activation functions.\n",
        "#Sigmoid Function , Tanh Function ,ReLU Function\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "x=np.array([0,-1.5,2,1])\n",
        "print('sigmoid funtion value =',tf.sigmoid(x))\n",
        "print('tanh function value =',tf.tanh(x))\n",
        "print('ReLU function value =',tf.nn.relu(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0Y_qhYljR29",
        "outputId": "98a8436f-4222-4b31-8cf2-e5f9ae6fa993"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sigmoid funtion value = tf.Tensor([0.5        0.18242552 0.88079708 0.73105858], shape=(4,), dtype=float64)\n",
            "tanh function value = tf.Tensor([ 0.         -0.90514825  0.96402758  0.76159416], shape=(4,), dtype=float64)\n",
            "ReLU function value = tf.Tensor([0. 0. 2. 1.], shape=(4,), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2b4217e"
      },
      "source": [
        "### Activation Function\n",
        "\n",
        "In the context of artificial neural networks, an **activation function** is a crucial component of an artificial neuron (or node). It determines whether a neuron should be 'activated' (fire) or not, based on the weighted sum of its inputs.\n",
        "\n",
        "Here's a breakdown of its importance and common types:\n",
        "\n",
        "**Purpose of Activation Functions:**\n",
        "\n",
        "1.  **Introduce Non-linearity:** Without activation functions, a neural network would simply be a series of linear transformations. This means it would only be able to learn linear mappings from inputs to outputs, severely limiting its ability to solve complex problems. Activation functions introduce non-linearity, allowing the network to learn and approximate complex, non-linear relationships in data.\n",
        "2.  **Determine Output:** The activation function takes the weighted sum of the inputs to a neuron and biases, and then produces an output that is passed to the next layer of neurons.\n",
        "3.  **Provide Training Stability:** Some activation functions (like ReLU and its variants) help to mitigate issues like the vanishing gradient problem during training, allowing deeper networks to be trained more effectively.\n",
        "\n",
        "**Common Types of Activation Functions:**\n",
        "\n",
        "1.  **Sigmoid Function (Logistic Function):**\n",
        "    *   **Formula:** `Ïƒ(x) = 1 / (1 + e^(-x))`\n",
        "    *   **Output Range:** (0, 1)\n",
        "    *   **Characteristics:** Smooth, differentiable, and often used in the output layer for binary classification problems (where the output needs to be a probability).\n",
        "    *   **Drawbacks:** Prone to vanishing gradient problem for very large or very small inputs, and outputs are not zero-centered.\n",
        "\n",
        "2.  **Tanh Function (Hyperbolic Tangent):**\n",
        "    *   **Formula:** `tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))`\n",
        "    *   **Output Range:** (-1, 1)\n",
        "    *   **Characteristics:** Similar to sigmoid but outputs are zero-centered, which can make training easier for subsequent layers.\n",
        "    *   **Drawbacks:** Still suffers from the vanishing gradient problem.\n",
        "\n",
        "3.  **ReLU (Rectified Linear Unit):**\n",
        "    *   **Formula:** `f(x) = max(0, x)`\n",
        "    *   **Output Range:** [0, infinity)\n",
        "    *   **Characteristics:** Simple, computationally efficient, and helps to alleviate the vanishing gradient problem. It's the most widely used activation function in hidden layers.\n",
        "    *   **Drawbacks:** Suffers from the 'dying ReLU' problem, where neurons can become inactive if their input is always negative, causing their gradient to be zero.\n",
        "\n",
        "4.  **Leaky ReLU:**\n",
        "    *   **Formula:** `f(x) = max(0.01x, x)` (for x < 0, output is 0.01x; for x >= 0, output is x)\n",
        "    *   **Output Range:** (-infinity, infinity)\n",
        "    *   **Characteristics:** Addresses the 'dying ReLU' problem by allowing a small, non-zero gradient when the input is negative.\n",
        "\n",
        "5.  **ELU (Exponential Linear Unit):**\n",
        "    *   **Formula:** `f(x) = x` if `x >= 0`, and `f(x) = alpha * (e^x - 1)` if `x < 0`\n",
        "    *   **Output Range:** (-alpha, infinity)\n",
        "    *   **Characteristics:** Can lead to faster learning and better performance than ReLU by producing negative outputs, which helps to push the mean activation closer to zero.\n",
        "\n",
        "6.  **Softmax Function:**\n",
        "    *   **Formula:** `softmax(x_i) = e^(x_i) / sum(e^x_j)`\n",
        "    *   **Output Range:** (0, 1) for each output, and the sum of all outputs is 1.\n",
        "    *   **Characteristics:** Typically used in the output layer of a multi-class classification neural network. It converts a vector of arbitrary real values into a probability distribution.\n",
        "\n",
        "Choosing the right activation function depends on the specific task and network architecture, but ReLU and its variants are generally good starting points for hidden layers."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#write a program to implement logic gate using MP Neuron model.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "x=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "w=np.array([1,0])\n",
        "yact=np.array([0,0,0,1])\n",
        "print('Input matrix:', x)\n",
        "print('Weight matrix:', w)\n",
        "print('Actual output matrix:', yact)\n",
        "y =np.dot(x,w.T)\n",
        "print('Total input :', y)\n",
        "for i in range(len(y)):\n",
        "  if(y[i]>=2):\n",
        "    y[i]=1\n",
        "  else:\n",
        "    y[i]=0\n",
        "print('Predicted output :', y)\n",
        "print('check for the correctness of predicted output :\\n', np.allclose(y,yact))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zodmTTjqq5S3",
        "outputId": "8b27ec43-63c8-4680-ef0d-ec49276b1907"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix: [[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "Weight matrix: [1 0]\n",
            "Actual output matrix: [0 0 0 1]\n",
            "Total input : [0 0 1 1]\n",
            "Predicted output : [0 0 0 0]\n",
            "check for the correctness of predicted output :\n",
            " False\n"
          ]
        }
      ]
    }
  ]
}